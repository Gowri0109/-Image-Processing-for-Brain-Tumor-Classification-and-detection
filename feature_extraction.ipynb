{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction from previous segmented images \n",
    "#to extract features for each image in the dataset such as texture, color, shape, size, etc\n",
    "#and store them in a file\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "import pywt\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "from skimage import io, feature, measure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.filters import gabor\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set the path to the folder containing the images\n",
    "folder_path = 'D:\\\\master_thesis\\\\datasets\\\\segmented\\\\normal'\n",
    "output_path = 'D:\\\\master_thesis\\\\datasets\\\\feature_extracted\\\\normal'\n",
    "# Define the parameters for the Gabor filter\n",
    "ksize = (31, 31)  # Size of the Gabor kernel\n",
    "sigma = 5.0  # Standard deviation of the Gaussian envelope\n",
    "theta = np.pi / 4  # Orientation of the Gabor filter\n",
    "lambd = 10.0  # Wavelength of the sinusoidal factor\n",
    "gamma = 0.5  # Spatial aspect ratio\n",
    "psi = 0  # Phase offset\n",
    "\n",
    "# Initialize a list to store the extracted features\n",
    "features = []\n",
    "\n",
    "# Iterate over each image in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Load the image\n",
    "    image_path = os.path.join(folder_path, filename)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Create the Gabor kernel\n",
    "    kernel = cv2.getGaborKernel(ksize, sigma, theta, lambd, gamma, psi, ktype=cv2.CV_32F)\n",
    "\n",
    "    # Apply the Gabor filter to the image\n",
    "    filtered_image = cv2.filter2D(image, cv2.CV_8UC3, kernel)\n",
    "\n",
    "    # Extract features from the filtered image\n",
    "    image_features = filtered_image.flatten()\n",
    "\n",
    "    # Append the features to the list\n",
    "    features.append(image_features)\n",
    "\n",
    "    # Save the extracted features to a file\n",
    "    feature_file_path = os.path.join(output_path, rf\"{filename}_features.npy\")\n",
    "    np.save(feature_file_path, image_features)\n",
    "\n",
    "# Convert the list of features to a numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Print the shape of the extracted features\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8226,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gowri\\AppData\\Local\\Temp\\ipykernel_2996\\2071071349.py:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  features = np.array(features)\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the folder containing the images\n",
    "folder_path = 'D:\\\\master_thesis\\\\datasets\\\\segmented\\\\pituitary'\n",
    "output_path = 'D:\\\\master_thesis\\\\datasets\\\\feature_extracted2\\\\pituitary'\n",
    "#csv_file_path = 'D:\\\\master_thesis\\\\datasets\\\\feature_extracted2\\\\glioma\\\\glioma.csv'\n",
    "\n",
    "# Define the parameters for the Gabor filter\n",
    "ksize = (31, 31)  # Size of the Gabor kernel\n",
    "sigma = 5.0  # Standard deviation of the Gaussian envelope\n",
    "theta = np.pi / 4  # Orientation of the Gabor filter\n",
    "lambd = 10.0  # Wavelength of the sinusoidal factor\n",
    "gamma = 0.5  # Spatial aspect ratio\n",
    "psi = 0  # Phase offset\n",
    "\n",
    "# Initialize a list to store the extracted features\n",
    "features = []\n",
    "\n",
    "# Iterate over each image in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Load the image\n",
    "    image_path = os.path.join(folder_path, filename)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Create the Gabor kernel\n",
    "    kernel = cv2.getGaborKernel(ksize, sigma, theta, lambd, gamma, psi, ktype=cv2.CV_32F)\n",
    "\n",
    "    # Apply the Gabor filter to the image\n",
    "    filtered_image = cv2.filter2D(image, cv2.CV_32F, kernel)\n",
    "\n",
    "    # Extract features from the filtered image\n",
    "    image_features = filtered_image.flatten()\n",
    "\n",
    "    # Append the features to the list\n",
    "    features.append(image_features)\n",
    "\n",
    "    # Save the extracted features to a file\n",
    "    feature_file_path = os.path.join(output_path, filename)\n",
    "    cv2.imwrite(feature_file_path, image_features)\n",
    "\n",
    "    # Print and visualize the Gabor filter\n",
    "    # plt.imshow(kernel, cmap='gray')\n",
    "    # plt.title('Gabor Filter')\n",
    "    # plt.show()\n",
    "\n",
    "# Convert the list of features to a numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Save the features in CSV format\n",
    "# with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "#     writer = csv.writer(csv_file)\n",
    "#     writer.writerows(features)\n",
    "\n",
    "# Print the shape of the extracted features\n",
    "print(features.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2396,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gowri\\AppData\\Local\\Temp\\ipykernel_22684\\472563600.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  features = np.array(features)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set the path to the folder containing the images\n",
    "folder_path = 'D:\\\\master_thesis\\\\datasets\\\\segmented\\\\normal'\n",
    "output_path = 'D:\\\\master_thesis\\\\datasets\\\\feature_extracted\\\\normal'\n",
    "# Define the parameters for the Gabor filter\n",
    "ksize = (31, 31)  # Size of the Gabor kernel\n",
    "sigma = 5.0  # Standard deviation of the Gaussian envelope\n",
    "theta = np.pi / 4  # Orientation of the Gabor filter\n",
    "lambd = 10.0  # Wavelength of the sinusoidal factor\n",
    "gamma = 0.5  # Spatial aspect ratio\n",
    "psi = 0  # Phase offset\n",
    "\n",
    "# Initialize a list to store the extracted features\n",
    "features = []\n",
    "\n",
    "# Iterate over each image in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Load the image\n",
    "    image_path = os.path.join(folder_path, filename)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Create the Gabor kernel\n",
    "    kernel = cv2.getGaborKernel(ksize, sigma, theta, lambd, gamma, psi, ktype=cv2.CV_32F)\n",
    "\n",
    "    # Apply the Gabor filter to the image\n",
    "    filtered_image = cv2.filter2D(image, cv2.CV_8UC3, kernel)\n",
    "\n",
    "    # Extract features from the filtered image\n",
    "    image_features = filtered_image.flatten()\n",
    "\n",
    "    # Append the features to the list\n",
    "    features.append(image_features)\n",
    "\n",
    "    # Save the extracted features to a file\n",
    "    feature_file_path = os.path.join(output_path, rf\"{filename}_features.npy\")\n",
    "    np.save(feature_file_path, image_features)\n",
    "\n",
    "# Convert the list of features to a numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Print the shape of the extracted features\n",
    "print(features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\torch\\torch_importer.cpp:1022: error: (-213:The function/feature is not implemented) Unsupported Lua type in function 'cv::dnn::dnn4_v20221220::TorchImporter::readObject'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m image_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mmaster_thesis\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\\\u001b[39m\u001b[39msegmented\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mglioma\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39m# Load the pre-trained VGG16 model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mdnn\u001b[39m.\u001b[39;49mreadNetFromTorch(\u001b[39m\"\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mmaster_thesis\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mdatasets\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39msegmented\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mglioma\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39marchive\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mvgg16_weights_tf_dim_ordering_tf_kernels.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Initialize lists to store the extracted features and labels\u001b[39;00m\n\u001b[0;32m     14\u001b[0m features \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\torch\\torch_importer.cpp:1022: error: (-213:The function/feature is not implemented) Unsupported Lua type in function 'cv::dnn::dnn4_v20221220::TorchImporter::readObject'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to the directory containing the MRI images\n",
    "image_dir = 'D:\\master_thesis\\datasets\\segmented\\glioma'\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "model = cv2.dnn.readNetFromTorch(\"D:\\\\master_thesis\\\\datasets\\\\segmented\\\\glioma\\\\archive\\\\vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\n",
    "\n",
    "# Initialize lists to store the extracted features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over each image file in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust file extensions if needed\n",
    "        # Load and preprocess the image\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        blob = cv2.dnn.blobFromImage(img, 1, (224, 224), (103.939, 116.779, 123.68), swapRB=True)\n",
    "\n",
    "        # Extract features using the VGG16 model\n",
    "        model.setInput(blob)\n",
    "        output = model.forward()\n",
    "        features.append(output.flatten())\n",
    "\n",
    "        # Extract the label from the filename or any other logic based on your data\n",
    "        label = filename.split('.')[0]  # Assuming filename format: label.jpg\n",
    "        labels.append(label)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Apply feature scaling\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=100)  # Adjust the number of components as per your requirements\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Save the extracted features and labels\n",
    "np.save('features.npy', features_pca)\n",
    "np.save('labels.npy', labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m plt\u001b[39m.\u001b[39mimshow(kernel)\n\u001b[0;32m     26\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(input_path) \u001b[39m#USe ksize:15, s:5, q:pi/2, l:pi/4, g:0.9, phi:0.8\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(img, cmap\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgray\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     29\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(img, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     30\u001b[0m fimg \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mfilter2D(img, cv2\u001b[39m.\u001b[39mCV_8UC3, kernel)\n",
      "File \u001b[1;32mc:\\Users\\Gowri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2689\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[0;32m   2690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[0;32m   2691\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2692\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2693\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[0;32m   2694\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2695\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[0;32m   2696\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[0;32m   2697\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[0;32m   2698\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[0;32m   2699\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n\u001b[0;32m   2700\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[0;32m   2701\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[0;32m   2702\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2703\u001b[0m     sci(__ret)\n\u001b[0;32m   2704\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\Gowri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\__init__.py:1472\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1470\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1471\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1472\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1474\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1475\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1476\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\Gowri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5665\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5657\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5658\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[0;32m   5659\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[0;32m   5660\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[0;32m   5661\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   5662\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5663\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 5665\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[0;32m   5666\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5667\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5668\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gowri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\image.py:701\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39msafe_masked_invalid(A, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    699\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[0;32m    700\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m--> 701\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m    704\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    705\u001b[0m     \u001b[39m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#garbor filter\n",
    "#input directory\n",
    "input_path = \"D:\\master_thesis\\datasets\\segmented\\glioma\"\n",
    "\n",
    "#output directory\n",
    "\n",
    "output_path = \"D:\\master_thesis\\datasets\\feature_extracted\\glioma\"\n",
    "\n",
    "#convert to grayscale\n",
    "#gray = cv2.cvtColor(input_path, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ksize = 25  #Use size that makes sense to the image and fetaure size. Large may not be good. \n",
    "#On the synthetic image it is clear how ksize affects imgae (try 5 and 50)\n",
    "sigma = 10 #Large sigma on small features will fully miss the features. \n",
    "theta = 1*np.pi/2  #/4 shows horizontal 3/4 shows other horizontal. Try other contributions\n",
    "lamda = 1*np.pi/4  #1/4 works best for angled. \n",
    "gamma=0.9  #Value of 1 defines spherical. Calue close to 0 has high aspect ratio\n",
    "#Value of 1, spherical may not be ideal as it picks up features from other regions.\n",
    "phi = 0.8  #Phase offset. I leave it to 0. (For hidden pic use 0.8)\n",
    "\n",
    "kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, phi, ktype=cv2.CV_32F)\n",
    "\n",
    "plt.imshow(kernel)\n",
    "\n",
    "\n",
    "img = cv2.imread(input_path) #USe ksize:15, s:5, q:pi/2, l:pi/4, g:0.9, phi:0.8\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "fimg = cv2.filter2D(img, cv2.CV_8UC3, kernel)\n",
    "\n",
    "kernel_resized = cv2.resize(kernel, (600, 600))                    # Resize image\n",
    "\n",
    "\n",
    "plt.imshow(kernel_resized)\n",
    "plt.imshow(fimg)\n",
    "\n",
    "cv2.imshow('Kernel', kernel_resized)\n",
    "cv2.imshow('Original Img.', img)\n",
    "cv2.imshow('Filtered', fimg)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "#\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m#img = cv2.imread('BSE_Image.jpg')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(input_path)\n\u001b[1;32m----> 4\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(img, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)  \n\u001b[0;32m      5\u001b[0m \u001b[39m#Here, if you have multichannel image then extract the right channel instead of converting the image to grey. \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m#For example, if DAPI contains nuclei information, extract the DAPI channel image first. \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[39m#Multiple images can be used for training. For that, you need to concatenate the data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[39m#Save original image pixels into a data frame. This is our Feature #1.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m img2 \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "input_path = \"D:\\master_thesis\\datasets\\segmented\\glioma\"\n",
    "#img = cv2.imread('BSE_Image.jpg')\n",
    "img = cv2.imread(input_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  \n",
    "#Here, if you have multichannel image then extract the right channel instead of converting the image to grey. \n",
    "#For example, if DAPI contains nuclei information, extract the DAPI channel image first. \n",
    "\n",
    "#Multiple images can be used for training. For that, you need to concatenate the data\n",
    "\n",
    "#Save original image pixels into a data frame. This is our Feature #1.\n",
    "img2 = img.reshape(-1)\n",
    "df = pd.DataFrame()\n",
    "df['Original Image'] = img2\n",
    "\n",
    "#Generate Gabor features\n",
    "num = 1  #To count numbers up in order to give Gabor features a lable in the data frame\n",
    "kernels = []  #Create empty list to hold all kernels that we will generate in a loop\n",
    "for theta in range(8):   #Define number of thetas. Here only 2 theta values 0 and 1/4 . pi \n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3, 5, 7):  #Sigma with values of 1 and 3\n",
    "        for lamda in np.arange(0, np.pi, np.pi / 4):   #Range of wavelengths\n",
    "            for gamma in (0.05, 0.5):   #Gamma values of 0.05 and 0.5\n",
    "                           \n",
    "                gabor_label = 'Gabor' + str(num)  #Label Gabor columns as Gabor1, Gabor2, etc.\n",
    "                print(gabor_label)\n",
    "                ksize=5  #Try 15 for hidden image. Or 9 for others\n",
    "                phi = 0  #0.8 for hidden image. Otherwise leave it to 0\n",
    "                kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, phi, ktype=cv2.CV_32F)    \n",
    "                kernels.append(kernel)\n",
    "                #Now filter the image and add values to a new column \n",
    "                fimg = cv2.filter2D(img2, cv2.CV_8UC3, kernel)                \n",
    "                filtered_img = fimg.reshape(-1)\n",
    "                \n",
    "                cv2.imwrite('images/gabor_filtered_images/'+gabor_label+'.jpg', filtered_img.reshape(img.shape))\n",
    "\n",
    "                df[gabor_label] = filtered_img  #Labels columns as Gabor1, Gabor2, etc.\n",
    "                print(gabor_label, ': theta=', theta, ': sigma=', sigma, ': lamda=', lamda, ': gamma=', gamma)\n",
    "                \n",
    "                num += 1  #Increment for gabor column label\n",
    "                \n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't parse 'ksize'. Expected sequence length 2, got 600",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m image_resized \u001b[39m=\u001b[39m resize(image, target_size)\n\u001b[0;32m     30\u001b[0m \u001b[39m#applying garbor filter\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m garbor_filter_real , garbor_filter_imag \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mgetGaborKernel(image_resized, frequency, theta, sigma, ksize, phi)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Flatten the Gabor filter responses into a feature vector\u001b[39;00m\n\u001b[0;32m     34\u001b[0m features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([garbor_filter_real\u001b[39m.\u001b[39mflatten(), garbor_filter_imag\u001b[39m.\u001b[39mflatten()])\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't parse 'ksize'. Expected sequence length 2, got 600"
     ]
    }
   ],
   "source": [
    "#Feture extraction using Garbor filter\n",
    "\n",
    "#defining parameters for the Garbor filter\n",
    "\n",
    "frequency = 0.6\n",
    "theta= 0.5\n",
    "sigma = 3.0\n",
    "ksize = 5\n",
    "phi = 0.0\n",
    "\n",
    "#input directory\n",
    "input_path = \"D:\\master_thesis\\datasets\\segmented\\glioma\"\n",
    "\n",
    "#list all the images in the path\n",
    "image_files = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "\n",
    "\n",
    "#initializing a list to store thr extracted features\n",
    "features = []\n",
    "\n",
    "#iterating over each file in input_path\n",
    "for file in image_files:\n",
    "    image_path = os.path.join(input_path, file)\n",
    "    image = io.imread(image_path, as_gray=True)\n",
    "\n",
    "    #resizing the image to a fixed size\n",
    "    target_size = (600,600)\n",
    "    image_resized = resize(image, target_size)\n",
    "\n",
    "    #applying garbor filter\n",
    "    garbor_filter_real , garbor_filter_imag = cv2.getGaborKernel(image_resized, frequency, theta, sigma, ksize, phi)\n",
    "\n",
    "    # Flatten the Gabor filter responses into a feature vector\n",
    "    features = np.concatenate([garbor_filter_real.flatten(), garbor_filter_imag.flatten()])\n",
    "\n",
    "    # Append the features to the list\n",
    "    features.append(features)\n",
    "\n",
    "# Convert the list of features to a numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Save the features to a CSV file\n",
    "output_file = 'D:\\master_thesis\\features.csv'\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 50 is different from 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m contours \u001b[39m=\u001b[39m measure\u001b[39m.\u001b[39mfind_contours(image, \u001b[39m0.5\u001b[39m) \n\u001b[0;32m     20\u001b[0m num_countours \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(contours)\n\u001b[1;32m---> 21\u001b[0m perimeter \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum([measure\u001b[39m.\u001b[39mperimeter(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m contours])\n\u001b[0;32m     23\u001b[0m \u001b[39m#texture features \u001b[39;00m\n\u001b[0;32m     24\u001b[0m lbp \u001b[39m=\u001b[39m feature\u001b[39m.\u001b[39mlocal_binary_pattern(image, P\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, R\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m contours \u001b[39m=\u001b[39m measure\u001b[39m.\u001b[39mfind_contours(image, \u001b[39m0.5\u001b[39m) \n\u001b[0;32m     20\u001b[0m num_countours \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(contours)\n\u001b[1;32m---> 21\u001b[0m perimeter \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum([measure\u001b[39m.\u001b[39;49mperimeter(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m contours])\n\u001b[0;32m     23\u001b[0m \u001b[39m#texture features \u001b[39;00m\n\u001b[0;32m     24\u001b[0m lbp \u001b[39m=\u001b[39m feature\u001b[39m.\u001b[39mlocal_binary_pattern(image, P\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, R\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gowri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\skimage\\measure\\_regionprops_utils.py:248\u001b[0m, in \u001b[0;36mperimeter\u001b[1;34m(image, neighborhood)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39m# You can also write\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39m# return perimeter_weights[perimeter_image].sum()\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[39m# but that was measured as taking much longer than bincount + np.dot (5x\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[39m# as much time)\u001b[39;00m\n\u001b[0;32m    247\u001b[0m perimeter_histogram \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(perimeter_image\u001b[39m.\u001b[39mravel(), minlength\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m total_perimeter \u001b[39m=\u001b[39m perimeter_histogram \u001b[39m@\u001b[39;49m perimeter_weights\n\u001b[0;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m total_perimeter\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 50 is different from 256)"
     ]
    }
   ],
   "source": [
    "# feature extraction using machine learning models -  Random forest classifier\n",
    "#load the input data\n",
    "input_data  = [\"D:\\master_thesis\\datasets\\segmented\\glioma\", \"D:\\master_thesis\\datasets\\segmented\\menin\",\"D:\\master_thesis\\datasets\\segmented\\pituitary\", \"D:\\master_thesis\\datasets\\segmented\\normal\"]\n",
    "\n",
    "#lists to store the extracted features and lables\n",
    "feature = []\n",
    "target_labels =[]\n",
    "\n",
    "#extract the features from each iamge in the dataset\n",
    "for folder_path in input_data:\n",
    "    label = os.path.basename(folder_path)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, file_name)\n",
    "        image = io.imread(image_path, as_gray=True)\n",
    "\n",
    "   \n",
    "\n",
    "    #shape features\n",
    "    contours = measure.find_contours(image, 0.5) \n",
    "    num_countours = len(contours)\n",
    "    perimeter = np.sum([measure.perimeter(c) for c in contours])\n",
    "\n",
    "    #texture features \n",
    "    lbp = feature.local_binary_pattern(image, P=8, R=1)\n",
    "    histogram = np.histogram(lbp, bins=256, range=(0,256))[0]\n",
    "    texture_features = histogram/np.sim(histogram)\n",
    "\n",
    "    #intensity features\n",
    "    mean_intensity = np.mean(image)\n",
    "    std_intensity = np.std(image)\n",
    "\n",
    "    #combine the extracted features into feature vector\n",
    "    feature_vector = [num_countours,perimeter] + texture_features.tolist()+ [mean_intensity,std_intensity]\n",
    "\n",
    "    #append the feature vector and label to repective labels\n",
    "    feature.append(feature_vector)\n",
    "    target_labels.append(label)\n",
    "\n",
    "#convert the features and label lists to numpy arrays\n",
    "X = np.array(feature)\n",
    "Y = np.array(target_labels)\n",
    "\n",
    "#split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#train a machine learning model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "#make predictions on the test set\n",
    "y_pred = model\n",
    "\n",
    "#evaluate the model\n",
    "accuray = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy: \",accuray)\n",
    "np.savetxt(\"D:\\master_thesis\\datasets\\segmented\\glioma\\feature_extraction.csv\",X,Y,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extractor using wavelets\n",
    "class feature_extract:\n",
    "    def __init__(self, wavelet= 'haar', level=1):\n",
    "        self.wavelet = wavelet\n",
    "        self.level = level\n",
    "\n",
    "    def  extract(self, image):\n",
    "        #convert image to float and scale to [0,1]\n",
    "        image = image.astype('float32')/255\n",
    "\n",
    "        #apply wavelet transform\n",
    "        coeffs = pywt.wavedec2(image, self.wavelet, level=self.level)\n",
    "        #extract features\n",
    "        features = []\n",
    "        for i, coeffs in enumerate(coeffs):\n",
    "            if i == 0:\n",
    "                #approximation coefficients\n",
    "                features += self._extract_coeff_features(coeffs, 'A')\n",
    "            \n",
    "            else:\n",
    "                #deatil coeeficients\n",
    "                features += self._extract_coeff_features(coeffs[0], 'H')\n",
    "                features += self._extract_coeff_features(coeffs[1], 'V')\n",
    "                features += self._extract_coeff_features(coeffs[2], 'D')\n",
    "\n",
    "        return features\n",
    "    \n",
    "    def _extract_coeff_features(self, coeff, label):\n",
    "        #calculate statistics for wavelet coefficients\n",
    "        features = []\n",
    "        features.append(np.mean(coeff))\n",
    "        features.append(np.std(coeff))\n",
    "        features.append(np.median(coeff))\n",
    "        features.append(np.max(coeff))\n",
    "        features.append(np.min(coeff))\n",
    "\n",
    "        # Add label to feature names\n",
    "        features = [f'{label}_{f}' for f in features]\n",
    "\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction using shape based\n",
    "\n",
    "class shape_features:\n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "\n",
    "    def extract_features(self, image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # detect and extract features\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "        faces = detector(gray)\n",
    "        for face in faces:\n",
    "            shape = predictor(gray, face)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            # loop over the (x, y)-coordinates for the facial landmarks\n",
    "            # and draw them on the image\n",
    "            for (x, y) in shape:\n",
    "                cv2.circle(image, (x, y), 1, (0, 0, 255), -1)\n",
    "        # show the output image with the face detections + facial landmarks\n",
    "        cv2.imshow(\"Output\", image)\n",
    "        cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
