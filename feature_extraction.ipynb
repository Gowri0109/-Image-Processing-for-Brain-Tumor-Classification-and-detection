{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction from previous segmented images \n",
    "#to extract features for each image in the dataset such as texture, color, shape, size, etc\n",
    "#and store them in a file\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "import pywt\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "from skimage import io, feature, measure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(labels)\n\u001b[0;32m     39\u001b[0m \u001b[39m#split the data into training and testing sets\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m X_train, X_test, Y_train, Y_test \u001b[39m=\u001b[39m train_test_split(X,Y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[0;32m     42\u001b[0m \u001b[39m#train a machine learning model\u001b[39;00m\n\u001b[0;32m     43\u001b[0m model \u001b[39m=\u001b[39m RandomForestClassifier()\n",
      "File \u001b[1;32mc:\\Users\\Gowri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m )\n\u001b[0;32m   2566\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Gowri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# feature extraction using machine learning models -  Random forest classifier\n",
    "#load the input data\n",
    "input_data  = [\"D:\\master_thesis\\datasets\\segmented\\glioma\", \"D:\\master_thesis\\datasets\\segmented\\menin\",\"D:\\master_thesis\\datasets\\segmented\\pituitary\", \"D:\\master_thesis\\datasets\\segmented\\normal\"]\n",
    "\n",
    "#lists to store the extracted features and lables\n",
    "feature = []\n",
    "labels =[]\n",
    "\n",
    "#extract the features from each iamge in the dataset\n",
    "for image_path, label in zip(input_data, labels):\n",
    "    image = io.imread(image_path, as_gray=True)\n",
    "    labels.append(label)\n",
    "\n",
    "    #shape features\n",
    "    contours = measure.find_contours(image, 0.5)\n",
    "    num_countours = len(contours)\n",
    "    perimeter = np.sum([measure.perimeter(c) for c in contours])\n",
    "\n",
    "    #texture features \n",
    "    lbp = feature.local_binary_pattern(image, P=8, R=1)\n",
    "    histogram = np.histogram(lbp, bins=256, range=(0,256))[0]\n",
    "    texture_features = histogram/np.sim(histogram)\n",
    "\n",
    "    #intensity features\n",
    "    mean_intensity = np.mean(image)\n",
    "    std_intensity = np.std(image)\n",
    "\n",
    "    #combine the extracted features into feature vector\n",
    "    feature_vector = [num_countours,perimeter] + texture_features.tolist()+ [mean_intensity,std_intensity]\n",
    "\n",
    "    #append the feature vector and label to repective labels\n",
    "    feature.append(feature_vector)\n",
    "    labels.append(label)\n",
    "\n",
    "#convert the features and label lists to numpy arrays\n",
    "X = np.array(feature)\n",
    "Y = np.array(labels)\n",
    "\n",
    "#split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#train a machine learning model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "#make predictions on the test set\n",
    "y_pred = model\n",
    "\n",
    "#evaluate the model\n",
    "accuray = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy: \",accuray)\n",
    "np.savetxt(\"D:\\master_thesis\\datasets\\segmented\\glioma\\feature_extraction.csv\",X,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extractor using wavelets\n",
    "class feature_extract:\n",
    "    def __init__(self, wavelet= 'haar', level=1):\n",
    "        self.wavelet = wavelet\n",
    "        self.level = level\n",
    "\n",
    "    def  extract(self, image):\n",
    "        #convert image to float and scale to [0,1]\n",
    "        image = image.astype('float32')/255\n",
    "\n",
    "        #apply wavelet transform\n",
    "        coeffs = pywt.wavedec2(image, self.wavelet, level=self.level)\n",
    "        #extract features\n",
    "        features = []\n",
    "        for i, coeffs in enumerate(coeffs):\n",
    "            if i == 0:\n",
    "                #approximation coefficients\n",
    "                features += self._extract_coeff_features(coeffs, 'A')\n",
    "            \n",
    "            else:\n",
    "                #deatil coeeficients\n",
    "                features += self._extract_coeff_features(coeffs[0], 'H')\n",
    "                features += self._extract_coeff_features(coeffs[1], 'V')\n",
    "                features += self._extract_coeff_features(coeffs[2], 'D')\n",
    "\n",
    "        return features\n",
    "    \n",
    "    def _extract_coeff_features(self, coeff, label):\n",
    "        #calculate statistics for wavelet coefficients\n",
    "        features = []\n",
    "        features.append(np.mean(coeff))\n",
    "        features.append(np.std(coeff))\n",
    "        features.append(np.median(coeff))\n",
    "        features.append(np.max(coeff))\n",
    "        features.append(np.min(coeff))\n",
    "\n",
    "        # Add label to feature names\n",
    "        features = [f'{label}_{f}' for f in features]\n",
    "\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction using shape based\n",
    "\n",
    "class shape_features:\n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "\n",
    "    def extract_features(self, image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # detect and extract features\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "        faces = detector(gray)\n",
    "        for face in faces:\n",
    "            shape = predictor(gray, face)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            # loop over the (x, y)-coordinates for the facial landmarks\n",
    "            # and draw them on the image\n",
    "            for (x, y) in shape:\n",
    "                cv2.circle(image, (x, y), 1, (0, 0, 255), -1)\n",
    "        # show the output image with the face detections + facial landmarks\n",
    "        cv2.imshow(\"Output\", image)\n",
    "        cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
